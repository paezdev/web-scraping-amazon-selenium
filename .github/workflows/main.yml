name: Pipeline de Scraping

on:
  push:
    branches:
      - main  # Ejecuta el pipeline al hacer push en la rama principal

jobs:
  run-scraping:
    runs-on: ubuntu-latest

    steps:
      # Paso 1: Clonar el código del repositorio
      - name: Checkout code
        uses: actions/checkout@v3

      # Paso 2: Configurar Python
      - name: Set up Python
        uses: actions/setup-python@v3
        with:
          python-version: '3.8'

      # Paso 3: Instalar dependencias
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install webdriver-manager  # Asegúrate de que webdriver-manager esté instalado

      # Paso 4: Instalar Brave Browser (si es necesario)
      - name: Install Brave Browser
        run: |
          sudo apt-get update
          sudo apt-get install -y apt-transport-https curl
          sudo curl -fsSLo /usr/share/keyrings/brave-browser-archive-keyring.gpg https://brave-browser-apt-release.s3.brave.com/brave-browser-archive-keyring.gpg
          echo "deb [signed-by=/usr/share/keyrings/brave-browser-archive-keyring.gpg] https://brave-browser-apt-release.s3.brave.com/ stable main" | sudo tee /etc/apt/sources.list.d/brave-browser-release.list
          sudo apt-get update
          sudo apt-get install -y brave-browser

      # Paso 5: Verificar que Brave está instalado
      - name: Check Brave Browser installation
        run: |
          which brave-browser
          brave-browser --version

      # Paso 6: Ejecutar el script de scraping
      - name: Run scraping script
        run: python PáezRamírez_JeanCarlos_Analizando_EA1.py

      # Paso 7: Ejecutar pruebas automatizadas con pytest
      - name: Run tests
        run: |
          pytest  # Asegúrate de que tu script tenga pruebas definidas en test_*.py

      # Paso 8: Subir el archivo generado como artefacto
      - name: Upload Data as Artifact
        uses: actions/upload-artifact@v3
        with:
          name: scraped-data
          path: output.csv

      # Paso 9: Hacer commit del archivo generado al repositorio
      - name: Push Results to Repository
        if: success()  # Solo ejecuta este paso si los pasos anteriores se completaron exitosamente
        env:
          SCRAPING: ${{ secrets.SCRAPING }}
        run: |
          git config --global user.name 'GitHub Actions'
          git config --global user.email 'actions@github.com'
          git remote set-url origin https://x-access-token:${SCRAPING}@github.com/${{ github.repository }}
          git add output.csv
          git commit -m "Datos extraídos automáticamente"
          git push origin main


