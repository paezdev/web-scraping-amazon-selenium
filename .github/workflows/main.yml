name: Pipeline de Scraping

on:
  push:
    branches:
      - main  # Ejecuta el pipeline al hacer push en la rama principal

jobs:
  run-scraping:
    runs-on: ubuntu-latest

    steps:
    # Paso 1: Clonar el código del repositorio
    - name: Checkout code
      uses: actions/checkout@v3

    # Paso 2: Configurar Python
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: '3.8'

    # Paso 3: Instalar dependencias
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    # Paso 4: Ejecutar pruebas automatizadas con pytest
    - name: Run tests
      run: pytest  # Asegúrate de que tu script tiene pruebas definidas en test_*.py

    # Paso 5: Ejecutar el script de scraping
    - name: Run scraping script
      run: python PáezRamírez_JeanCarlos_Analizando_EA1.py  # Cambia esto al nombre real de tu script

    # Paso 6: Subir el archivo generado como artefacto
    - name: Upload Data as Artifact
      uses: actions/upload-artifact@v3
      with:
        name: scraped-data
        path: output.csv  # Cambia esto al nombre del archivo que genera tu script

    # Paso 7 (opcional): Hacer commit del archivo generado al repositorio
    - name: Push Results to Repository
      if: success()  # Solo ejecuta este paso si los pasos anteriores se completaron exitosamente
      run: |
        git config --global user.name 'GitHub Actions'
        git config --global user.email 'actions@github.com'
        git add output.csv
        git commit -m "Datos extraídos automáticamente"
        git push origin main